{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Story generator",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NreMuR_wUa3"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils \n",
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from textwrap3 import wrap\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "# from langdetect import detect\n",
        "import nltk\n",
        "warnings.filterwarnings('ignore')\n",
        "import re\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTrDCP0Dwql7"
      },
      "source": [
        "df=pd.read_csv(\"https://raw.githubusercontent.com/jessiececilya/Projects/main/hippoCorpusV2.csv\")\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7kPwtXLP667"
      },
      "source": [
        "Convert the summary of the story into a list of arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKDE181n0FiT"
      },
      "source": [
        "corpus=df['summary'].to_list()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC70vpE-QG05"
      },
      "source": [
        "Cleaning of text data of puntuations, and ignoring any non ascii charchters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EStHRSLN0hn5"
      },
      "source": [
        "\n",
        "def text_cleaner(text):\n",
        "    text = \"\".join(car for car in text if car not in string.punctuation).lower()\n",
        "    text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
        "    return text\n",
        "\n",
        "corpus = [text_cleaner(line) for line in corpus]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6IxLvb2Qbo7"
      },
      "source": [
        "Considering first 5000 stories for the model and tokenizing the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3GKMu9k0lW3",
        "outputId": "eab8e796-e4f6-475b-eb44-cce63df36a1f"
      },
      "source": [
        "corpus = corpus[:5000]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "word_index = tokenizer.word_index\n",
        "total_words = len(word_index) + 1\n",
        "total_words"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdCyEW-RQpKk"
      },
      "source": [
        "Creating n-gram sentence sequence. This is to create a pattern to train the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTDvdxx_0qEr"
      },
      "source": [
        "input_sequences =[]\n",
        "\n",
        "for sentence in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxfNi9pns8w6"
      },
      "source": [
        "input_sequences=input_sequences[:30000]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjEjdXnTUDr3"
      },
      "source": [
        "Padding the input sequences upto the maximum length of the squence so as to train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJJ46WVZ0sv7"
      },
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, \n",
        "                                         maxlen=max_sequence_len, \n",
        "                                         padding='pre'))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0v6q6aBUOvZ"
      },
      "source": [
        "Separating the input and ouput tokens, where n-1 words are trained to predict nth word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfa41PZ60vaG"
      },
      "source": [
        "# create predictors and label\n",
        "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "# create one-hot encoding of the labels\n",
        "label = tensorflow.keras.utils.to_categorical(label, num_classes=total_words)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b7KIZNaxrWr",
        "outputId": "42f3e08c-b4e6-40db-a7cf-076fd0767a0f"
      },
      "source": [
        "input_sequences.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 63)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0rs2Jx6xu07",
        "outputId": "30a013f4-e8c6-4173-e780-7716f2d511a7"
      },
      "source": [
        "print(label[0])\n",
        "print(label[0].shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "(5909,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkKc_CMwUz5p"
      },
      "source": [
        "Creating the model using Birectional LSTM as it works better in sequential classification. I have introduced a drop out in order introduce fairness to neuron intensity. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-rBtnw31Lss",
        "outputId": "89c17e5e-3d49-4add-c0be-ab97d4ab27a7"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(200)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 62, 10)            59090     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 400)               337600    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5909)              2369509   \n",
            "=================================================================\n",
            "Total params: 2,766,199\n",
            "Trainable params: 2,766,199\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sab2l66YsrxP"
      },
      "source": [
        "history = model.fit(predictors, label, epochs=100,  verbose=1)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd1jf3ad2El4"
      },
      "source": [
        "input_text = \"A month ago\"\n",
        "input=input_text\n",
        "next_words = 50\n",
        "  \n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict_classes(token_list, verbose=0)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    input_text += \" \" + output_word\n",
        "\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SxuQTmOiH7Q",
        "outputId": "2ba9c624-e13e-4bde-c3a8-4d3382bd55a1"
      },
      "source": [
        "print(\"Input text: \",input)\n",
        "print(\"Output text: \")\n",
        "print(\"\\n\".join(wrap(input_text, width=50)))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input text:  A month ago\n",
            "Output text: \n",
            "A month ago of mothers and the excitement for the\n",
            "milestone transition from toddler to grade\n",
            "schooler and got up with pay and find the best\n",
            "time there is very be and i got to her they coming\n",
            "able of the lives this was never food that the\n",
            "promotion and was extremely scary\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU2bbfqhTwDx"
      },
      "source": [
        "model=tf.keras.models.load_model('/content/storymodel (2).h5')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCOFEIq9qJ0z"
      },
      "source": [
        "# !pip install textwrap3"
      ],
      "execution_count": 82,
      "outputs": []
    }
  ]
}